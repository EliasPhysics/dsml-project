{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import utils\n",
    "from model import TimeSeriesTransformer\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import hyperparameters\n",
    "from psd import power_spectrum_error, compute_power_spectrum\n",
    "from torch import nn, Tensor\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "from typing import Tuple\n",
    "import csv\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "First, we define the dataset class, that prepares training data from the long trajectory provided:",
   "id": "9e69fcdbcb8b4c8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TransformerDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 data: torch.tensor,\n",
    "                 indices: list,\n",
    "                 enc_seq_len: int,\n",
    "                 dec_seq_len: int,\n",
    "                 target_seq_len: int\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        self.indices = indices\n",
    "        self.data = data\n",
    "        self.enc_seq_len = enc_seq_len\n",
    "        self.dec_seq_len = dec_seq_len\n",
    "        self.target_seq_len = target_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns a tuple with 3 elements:\n",
    "        1) src (the encoder input)\n",
    "        2) trg (the decoder input)\n",
    "        3) trg_y (the target)\n",
    "        \"\"\"\n",
    "        # Get the first element of the i'th tuple in the list self.indicesasdfas\n",
    "        start_idx = self.indices[index][0]\n",
    "\n",
    "        # Get the second (and last) element of the i'th tuple in the list self.indices\n",
    "        end_idx = self.indices[index][1]\n",
    "\n",
    "        sequence = self.data[start_idx:end_idx]\n",
    "\n",
    "        src, trg, trg_y = self.get_src_trg(\n",
    "            sequence=sequence,\n",
    "            enc_seq_len=self.enc_seq_len,\n",
    "            dec_seq_len=self.dec_seq_len,\n",
    "            target_seq_len=self.target_seq_len\n",
    "        )\n",
    "\n",
    "        return src, trg, trg_y\n",
    "\n",
    "    def get_src_trg(\n",
    "            self,\n",
    "            sequence: torch.Tensor,\n",
    "            enc_seq_len: int,\n",
    "            dec_seq_len: int,\n",
    "            target_seq_len: int\n",
    "    ) -> Tuple[torch.tensor, torch.tensor, torch.tensor]:\n",
    "        \n",
    "        assert len(\n",
    "            sequence) == enc_seq_len + target_seq_len, \"Sequence length does not equal (input length + target length)\"\n",
    "\n",
    "        # encoder input\n",
    "        src = sequence[:enc_seq_len]\n",
    "        trg = sequence[enc_seq_len - 1:len(sequence) - 1]\n",
    "        assert len(trg) == target_seq_len, \"Length of trg does not match target sequence length\"\n",
    "\n",
    "        # The target sequence against which the model output will be compared to compute loss\n",
    "        trg_y = sequence[-target_seq_len:]\n",
    "        assert len(trg_y) == target_seq_len, \"Length of trg_y does not match target sequence length\"\n",
    "\n",
    "        return src, trg, trg_y"
   ],
   "id": "7aee9b7b9c55632e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we define the TimeSeriesTransformer",
   "id": "e6a67a178cb823ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_size: int,\n",
    "                 dec_seq_len: int,\n",
    "                 d_model: int = 512,\n",
    "                 n_encoder_layers: int = 4,\n",
    "                 n_decoder_layers: int = 4,\n",
    "                 dropout: float = 0.2,\n",
    "                 max_seq_len: int = 512,\n",
    "                 dim_feedforward_encoder: int = 2048,\n",
    "                 n_heads: int = 8,\n",
    "                 dim_feedforward_decoder: int = 2048,\n",
    "                 num_predicted_features: int = 3\n",
    "                 ):\n",
    "\n",
    "        super().__init__()\n",
    "        # positional encoder\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # create array for positional encoding\n",
    "        position_counter = torch.arange(max_seq_len).unsqueeze(1)\n",
    "        # taken from the positional encoding torch tutorial\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # create positional encoding shift to add to the sequential data\n",
    "        pos_encoding = torch.zeros(1, max_seq_len, d_model)\n",
    "        pos_encoding[0, :, 0::2] = torch.sin(position_counter * div_term)\n",
    "        pos_encoding[0, :, 1::2] = torch.cos(position_counter * div_term)\n",
    "\n",
    "        # this makes torch register the positional encoding as non-trainable parameter\n",
    "        self.register_buffer('pe', pos_encoding)\n",
    "\n",
    "        # encoder\n",
    "\n",
    "\n",
    "        self.encoder_input_layer = nn.Linear(\n",
    "            in_features=input_size,\n",
    "            out_features=d_model\n",
    "        )\n",
    "\n",
    "        # Create positional encoder from other module\n",
    "\n",
    "        # now build encoder layer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead = n_heads,\n",
    "            dim_feedforward=dim_feedforward_encoder,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # stack encoder layers to obtain Encoder\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=encoder_layer,\n",
    "            num_layers=n_encoder_layers,\n",
    "            norm=None\n",
    "        )\n",
    "\n",
    "        self.dec_seq_len = dec_seq_len\n",
    "\n",
    "        self.decoder_input_layer = nn.Linear(\n",
    "            in_features=num_predicted_features,\n",
    "            out_features=d_model,\n",
    "        )\n",
    "\n",
    "        # create one decoder layer\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward_decoder,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # stack the decoder layers\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            decoder_layer=decoder_layer,\n",
    "            num_layers=n_decoder_layers,\n",
    "            norm=None\n",
    "        )\n",
    "\n",
    "        self.linear_mapping = nn.Linear(\n",
    "            in_features=d_model,\n",
    "            out_features=num_predicted_features\n",
    "        )\n",
    "\n",
    "    def pos_encoding(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, enc_seq_len, dim_val]\n",
    "\n",
    "        returns: Tensor, shape [batch_size,enc_seq_len, dim_val]\n",
    "        \"\"\"\n",
    "        #print(f\"forward tensor shape: {x.shape}, pos enc shape: {self.pe[:, :x.size(1)].shape}\")\n",
    "        x = x + self.pe[:,:x.size(1)]\n",
    "\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor, src_mask: Tensor=None, tgt_mask: Tensor=None) -> Tensor:\n",
    "\n",
    "        src = self.encoder_input_layer(src)\n",
    "\n",
    "        # Pass through the positional encoding layer\n",
    "        # src = src.unsqueeze(0)\n",
    "        pos_encoded_src = self.pos_encoding(src)\n",
    "\n",
    "        encoder_output = self.encoder(\n",
    "            src=pos_encoded_src\n",
    "        )\n",
    "\n",
    "        tgt = self.decoder_input_layer(tgt)\n",
    "\n",
    "        decoder_output = self.decoder(\n",
    "            tgt=tgt,\n",
    "            memory=encoder_output,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=src_mask\n",
    "        )\n",
    "\n",
    "        decoder_output = self.linear_mapping(decoder_output)\n",
    "\n",
    "        return decoder_output"
   ],
   "id": "f4f0ac2a6ffbd704"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now the training procedure of the neural network",
   "id": "248c29e517086c9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_TimeSeriesTransformer(data_path, args):\n",
    "    # Training parameters\n",
    "    epochs = args[\"epochs\"]\n",
    "    batch_size = args[\"batch_size\"]\n",
    "\n",
    "    # Initialize data\n",
    "    data = utils.read_data(data_path)\n",
    "\n",
    "    ## Params from args\n",
    "    dec_seq_len = args[\"dec_seq_len\"]\n",
    "    enc_seq_len = args[\"enc_seq_len\"]\n",
    "    output_seq_len = args[\"output_seq_len\"]\n",
    "    window_size = args[\"window_size\"]\n",
    "    step_size = args[\"step_size\"]\n",
    "\n",
    "\n",
    "    training_indices = utils.get_indices_entire_sequence(\n",
    "        data=data,\n",
    "        window_size=window_size,\n",
    "        step_size=step_size)\n",
    "\n",
    "    training_data = TransformerDataset(data=data,\n",
    "                                     indices=training_indices,\n",
    "                                     enc_seq_len=enc_seq_len,\n",
    "                                     dec_seq_len=dec_seq_len,\n",
    "                                     target_seq_len=output_seq_len)\n",
    "\n",
    "    training_data = DataLoader(training_data, batch_size,shuffle=True)\n",
    "\n",
    "    model = TimeSeriesTransformer(\n",
    "        input_size=data.shape[1],  # Assuming 'data' is already loaded\n",
    "        dec_seq_len=args[\"dec_seq_len\"],\n",
    "        d_model=args[\"dim_val\"],\n",
    "        n_encoder_layers=args[\"n_encoder_layers\"],\n",
    "        n_decoder_layers=args[\"n_decoder_layers\"],\n",
    "        dropout=0.2,\n",
    "        max_seq_len=args[\"max_seq_len\"],\n",
    "        dim_feedforward_encoder=args[\"in_features_encoder_linear_layer\"],\n",
    "        n_heads=args[\"n_heads\"],\n",
    "        dim_feedforward_decoder=args[\"in_features_decoder_linear_layer\"],\n",
    "        num_predicted_features= 3 # Assuming prediction targets match input features\n",
    "    )\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(params=model.parameters())\n",
    "    criterion = torch.nn.HuberLoss()\n",
    "\n",
    "    # Make src mask for decoder with size:\n",
    "    # [batch_size*n_heads, output_sequence_length, enc_seq_len]\n",
    "    src_mask = utils.generate_square_subsequent_mask(\n",
    "        dim1=output_seq_len,\n",
    "        dim2=enc_seq_len\n",
    "        )\n",
    "\n",
    "    # Make tgt mask for decoder with size:\n",
    "    # [batch_size*n_heads, output_sequence_length, output_sequence_length]\n",
    "    tgt_mask = utils.generate_square_subsequent_mask(\n",
    "        dim1=output_seq_len,\n",
    "        dim2=output_seq_len\n",
    "        )\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    # Iterate over all epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "\n",
    "        # Iterate over all (x,y) pairs in training dataloader\n",
    "        for i, (src, tgt, tgt_y) in enumerate(training_data):\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            #print(src.shape, tgt.shape)\n",
    "\n",
    "            # Make forecasts\n",
    "            #print(f\"src: {src.shape}, tgt: {tgt.shape}\")\n",
    "            prediction = model(src=src, tgt=tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "\n",
    "            # Compute and backprop loss\n",
    "            loss = criterion(tgt_y, prediction)\n",
    "            losses.append(loss.detach())\n",
    "\n",
    "            loss.backward()\n",
    "            #print(loss.detach())\n",
    "\n",
    "            # Take optimizer step\n",
    "            optimizer.step()\n",
    "\n",
    "        # Iterate over all (x,y) pairs in validation dataloader\n",
    "\n",
    "\n",
    "\n",
    "    model_name = args[\"model_name\"]\n",
    "    torch.save(model.state_dict(), f\"models/{model_name}.pth\")\n",
    "\n",
    "    plt.plot(range(len(losses)),losses)\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.savefig(f\"plots/training_{model_name}.png\")\n",
    "\n",
    "    # Save hyperparameters as CSV\n",
    "    with open(f\"models/{model_name}.csv\", \"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Name\", \"Age\"])  # Column headers\n",
    "        for key, value in args.items():\n",
    "            writer.writerow([key, value])"
   ],
   "id": "c8e5882236c7bf21"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
